{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18350,
     "status": "ok",
     "timestamp": 1755551791910,
     "user": {
      "displayName": "Behafarin Emam",
      "userId": "09534945409115305507"
     },
     "user_tz": 240
    },
    "id": "TPCSmMxjdtE-",
    "outputId": "c5fae2e5-0c7c-4d0a-ba01-b773dd1b2b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here's a poem about open source machine learning in the style of E. E. Cummings:\n",
      "\n",
      "open source machine learning\n",
      "a gift to the world, a treasure trove\n",
      "of knowledge, freely shared\n",
      "a collaborative effort, a symphony\n",
      "\n",
      "of code and data, a work of art\n",
      "a journey to the future, a new start\n",
      "a chance to learn, to grow, to explore\n",
      "a community of minds, a diverse score\n",
      "\n",
      "the algorithms, they dance and play\n",
      "a complex melody, a beautiful array\n",
      "of patterns, of insights, of predictions\n",
      "a world of possibilities, a new dimension\n",
      "\n",
      "the models, they learn and adapt\n",
      "a living, breathing, ever-changing fact\n",
      "a tool for innovation, a key to unlock\n",
      "a door to the future, a new path to follow\n",
      "\n",
      "the code, it's open and free\n",
      "a gift to the world, a breeze\n",
      "to use, to modify, to share\n",
      "a movement, a revolution, a new era\n",
      "\n",
      "join the community, be a part\n",
      "of something bigger, of a new start\n",
      "together we'll shape the future, we'll make\n",
      "a better world, a brighter day to break."
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "my_token = \"Your API Key\"\n",
    "client = replicate.Client(api_token = my_token)\n",
    "\n",
    "# The meta/llama-2-70b-chat model can stream output as it's running.\n",
    "for event in client.stream(\n",
    "    \"meta/llama-2-70b-chat\",\n",
    "    input={\n",
    "        \"top_k\": 0,\n",
    "        \"top_p\": 1,\n",
    "        \"prompt\": \"Can you write a poem about open source machine learning? Let's make it in the style of E. E. Cummings.\",\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "        \"system_prompt\": \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\",\n",
    "        \"length_penalty\": 1,\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"min_new_tokens\": -1,\n",
    "        \"prompt_template\": \"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{prompt} [/INST]\",\n",
    "        \"presence_penalty\": 0,\n",
    "        \"log_performance_metrics\": False\n",
    "    },\n",
    "):\n",
    "    print(str(event), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ_6B-7CCZl7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import replicate\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 691454,
     "status": "error",
     "timestamp": 1755552610490,
     "user": {
      "displayName": "Behafarin Emam",
      "userId": "09534945409115305507"
     },
     "user_tz": 240
    },
    "id": "pP21RuBriM-J",
    "outputId": "12918ac9-231b-4beb-8413-459ad7f79052"
   },
   "outputs": [],
   "source": [
    "# --- 1. SETUP ---\n",
    "# The script will automatically use the REPLICATE_API_TOKEN environment variable.\n",
    "# Ensure it is set before running.\n",
    "# You can check if the token is available with:\n",
    "# print(os.getenv(\"REPLICATE_API_TOKEN\"))\n",
    "\n",
    "# --- 2. LOAD DATASET ---\n",
    "# Load the 'generation' subset of the TruthfulQA dataset\n",
    "print(\"Loading TruthfulQA dataset...\")\n",
    "dataset = load_dataset(\"truthful_qa\", \"generation\")\n",
    "# We'll use the validation split for this example\n",
    "questions = dataset['validation']['question']\n",
    "print(f\"Dataset loaded. Found {len(questions)} questions.\")\n",
    "\n",
    "# --- 3. DEFINE PARAMETERS ---\n",
    "# Define the temperatures to test\n",
    "temperatures = [0.1, 0.4, 0.7, 1.0, 1.3]\n",
    "\n",
    "# Llama 2 70B Chat model on Replicate\n",
    "LLAMA_2_70B_CHAT = \"meta/llama-2-70b-chat\"\n",
    "\n",
    "# List to store all the results\n",
    "results_list = []\n",
    "\n",
    "# --- 4. MAIN PROCESSING LOOP ---\n",
    "print(\"Starting to process questions with different temperatures...\")\n",
    "\n",
    "# You can limit the number of questions for a quick test, e.g., questions[:10]\n",
    "for i, question in enumerate(questions):\n",
    "    for temp in temperatures:\n",
    "        try:\n",
    "            print(f\"Processing Q{i+1}/{len(questions)}: '{question[:50]}...' with temp={temp}\")\n",
    "\n",
    "            # The system prompt guides the model's behavior\n",
    "            system_prompt = (\n",
    "                \"You are a helpful, respectful and honest assistant. \"\n",
    "                \"Always answer as helpfully as possible, while being safe. \"\n",
    "                \"Keep your answer concise, to a maximum of 5 sentences.\"\n",
    "            )\n",
    "\n",
    "            # The full prompt template\n",
    "            prompt_template = (\n",
    "                f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n\"\n",
    "                f\"{question} [/INST]\"\n",
    "            )\n",
    "\n",
    "            # Call the Replicate API\n",
    "            output = client.run(\n",
    "                LLAMA_2_70B_CHAT,\n",
    "                input={\n",
    "                    \"prompt\": prompt_template,\n",
    "                    \"temperature\": temp,\n",
    "                    \"max_new_tokens\": 150, # Limit output length\n",
    "                    \"min_new_tokens\": -1\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # The output is an iterator, so we join it to get the full string\n",
    "            full_response = \"\".join(list(output))\n",
    "\n",
    "            # Store the result\n",
    "            results_list.append({\n",
    "                \"Question\": question,\n",
    "                \"Temperature\": temp,\n",
    "                \"Answer\": full_response.strip()\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with question '{question}' at temp {temp}: {e}\")\n",
    "            # Optionally, store error information\n",
    "            results_list.append({\n",
    "                \"Question\": question,\n",
    "                \"Temperature\": temp,\n",
    "                \"Answer\": f\"API_ERROR: {e}\"\n",
    "            })\n",
    "\n",
    "# --- 5. SAVE RESULTS TO CSV ---\n",
    "print(\"\\nProcessing complete. Saving results to CSV file...\")\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(results_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_filename = 'truthfulqa_results.csv'\n",
    "df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Successfully saved results to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbkZgw1_CrO-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNTtU/r4RnjwzdfG7mZAboM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
