{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4db8874",
   "metadata": {},
   "source": [
    "# Environment Setup, Authentication and Acquisition\n",
    "\n",
    "Welcome to the project onboarding notebook. This notebook helps you configure your local environment and validate access to required services such as Google Cloud (BigQuery, Cloud Storage).\n",
    "\n",
    "One key step in this setup involves authenticating with Google Cloud using a **Service Account**. Each teammate will need access to a JSON key file for the project's service account that authenticates their access to shared cloud resources.\n",
    "\n",
    "Your service account key file should be placed in the credentials folder. This keeps sensitive files organized and makes it easier to manage your environment setup across machines or users.\n",
    "\n",
    "> **Important:** Never commit your service account JSON file to version control. The `.gitignore` includes all file found within the `credentials/` directory so `.env` files and the JSON key files will not be pushed to the public repo.\n",
    "\n",
    "The next section will create and validate a `.env` file that stores the path to your service account credentials and confirms successful authentication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f0233",
   "metadata": {},
   "source": [
    "## GCP Authentication & `.env` Setup\n",
    "\n",
    "This code block does the following:\n",
    "\n",
    "1. Checks whether `credentials/secrets.env` exists.\n",
    "2. If missing, it creates a **template** with a placeholder for your service account key.\n",
    "3. It then attempts to load the environment variable `GOOGLE_APPLICATION_CREDENTIALS` from the file.\n",
    "4. If a valid path is found and the file exists, it initializes your GCP clients (BigQuery, Cloud Storage) and prints your authenticated service account email.\n",
    "\n",
    "> If the `.env` file is missing, the script will create it and **halt execution**, allowing you to add your credentials before continuing. Once the `.env` file is created, add the full path to your JSON key file which should also be stored in the `credentials/` directory.\n",
    "\n",
    "Once authenticated, you can begin querying BigQuery or interacting with GCS buckets programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c61f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from google.cloud import storage, bigquery\n",
    "from google.auth import default\n",
    "from data_pipeline.uploader import DataUploader\n",
    "from data_acquisition.loader import main as run_loader\n",
    "from data_acquisition.data_cleaner import main as run_cleaner\n",
    "from data_acquisition.web_scraper import WebScraper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Authentication & `.env` Setup\n",
    "# This script sets up Google Cloud authentication and checks for the necessary environment variables.\n",
    "\n",
    "# Ensure credentials directory exists\n",
    "credentials_dir = Path(\"../credentials\")\n",
    "\n",
    "if not credentials_dir.exists():\n",
    "    print(\"Credentials directory not found. Creating...\")\n",
    "    credentials_dir.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    print(f\"Credentials directory found at: {credentials_dir.resolve()}\")\n",
    "    \n",
    "# Define secrets file path\n",
    "secrets_path = Path(\"../credentials/secrets.env\")\n",
    "\n",
    "# Create file if it doesn't exist\n",
    "if not secrets_path.exists():\n",
    "    print(\"'secrets.env' not found. Creating a template...\")\n",
    "    secrets_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    secrets_path.write_text(\"GOOGLE_APPLICATION_CREDENTIALS=path/to/your/service_account.json\\n\")\n",
    "    print(f\"Created template at: {secrets_path.resolve()}\")\n",
    "    print(\"Please update this file with the directory path to your GCP JSON key.\")\n",
    "    print(\"Store JSON key in the 'credentials' directory to prevent upload to GitHub.\")\n",
    "    sys.exit(1)  \n",
    "else:\n",
    "    print(f\"Found existing secrets file at: {secrets_path.resolve()}\")\n",
    "    \n",
    "load_dotenv(find_dotenv(\"../credentials/secrets.env\"))\n",
    "\n",
    "cred_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "if not cred_path or not os.path.exists(cred_path):\n",
    "    print(\n",
    "        \"GOOGLE_APPLICATION_CREDENTIALS is not set or the file does not exist.\\n\"\n",
    "        \"Please ensure secrets.env contains a valid path to your service account JSON file.\"\n",
    "    )\n",
    "    storage_client = None\n",
    "    bq_client = None\n",
    "else:\n",
    "    print(\"GOOGLE_APPLICATION_CREDENTIALS loaded from .env\")\n",
    "\n",
    "    # Initialize GCP clients using ADC\n",
    "    storage_client = storage.Client()\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Confirm authentication\n",
    "    creds, project_id = default()\n",
    "    member_email = creds.service_account_email\n",
    "    print(f\"Authenticated as: {member_email}\")\n",
    "    print(f\"GCP Project ID: {project_id}\")\n",
    "\n",
    "# GCP configuration\n",
    "REGION = \"us-east1\"\n",
    "print(f\"GCP region set to: {REGION}\")\n",
    "\n",
    "# Initialize GCP clients with the project ID if provided\n",
    "storage_client = storage.Client(project = project_id if project_id else None)\n",
    "bq_client = bigquery.Client(project = project_id if project_id else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753d6a2",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "This section outlines the process used to gather, standardize, and prepare a diverse set of question-answering (QA) datasets for downstream machine learning tasks.\n",
    "\n",
    "Our goal is to build a robust and scalable pipeline for retrieving raw datasets, performing initial validation and cleaning, and outputting schema-consistent files ready for preprocessing, exploration, feature extraction, and modeling.\n",
    "\n",
    "### Approach Overview\n",
    "\n",
    "The pipeline is composed of three main stages:\n",
    "\n",
    "1. **Data Loading** (`loader.py`):  \n",
    "   Downloads raw datasets from either direct URLs or the Hugging Face Hub. All files are stored in `/data/raw/` in their original formats (e.g., JSONL, CSV, Parquet).\n",
    "\n",
    "2. **Data Cleaning** (`cleaner.py`):  \n",
    "   Transforms raw files into clean, flat CSVs with standardized fields required for QA tasks: `id`, `title`, `context`, `question`, and `answers`. Rows with formatting or structural issues are logged separately for inspection.\n",
    "\n",
    "3. **Data Upload** (`#TODO - script in progress`):  \n",
    "   A final upload step will push cleaned datasets to BigQuery for centralized cloud storage, enabling streamlined access to modeling workflows using Google's ML tools, including Vertex AI.\n",
    "\n",
    "The pipeline is modular by design. New datasets can be added by extending the loader configuration and creating a dataset-specific cleaner as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fb449",
   "metadata": {},
   "source": [
    "## Run the Data Loading Script\n",
    "\n",
    "The `loader.py` script is responsible for downloading and storing a core set of fact verification and QA datasets into the local project environment in their **original file formats** (e.g., `.json`, `.jsonl`, `.parquet`, or `.csv`).\n",
    "\n",
    "Currently supported datasets include:\n",
    "- **FEVER 2.0**\n",
    "- **HotpotQA**\n",
    "- **Natural Questions (Lite)**\n",
    "- **SQuAD v2.0**\n",
    "- **TruthfulQA**\n",
    "\n",
    "The script is built around a modular `DataDownloader` class, which encapsulates:\n",
    "- dataset-specific retrieval logic,\n",
    "- support for both **Hugging Face Hub** and **direct download URLs**,\n",
    "- dynamic filetype handling for JSON, JSONL, CSV, and Parquet,\n",
    "- customizable storage paths.\n",
    "\n",
    "This design makes it easy to extend with new datasets: simply update the Hugging Face or URL mappings in `loader.py`, and rerun the script. Each dataset is downloaded only once unless the `overwrite` flag is enabled.\n",
    "\n",
    "> **Note:** All files are saved into the `/data/raw/` folder using consistent and identifiable filenames to support reproducibility and transparent data lineage in downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c865806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download raw data files from URLs and Hugging Face\n",
    "\n",
    "run_loader(force=False, prompt_user=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b054b13",
   "metadata": {},
   "source": [
    "## Run the Dataset Cleaning Script\n",
    "\n",
    "The `cleaner.py` script processes raw QA datasets from the `/data/raw/` directory and transforms them into clean, BigQuery-compatible CSV files stored in `/data/clean/`.\n",
    "\n",
    "This script uses the `DataCleaner` class, which includes dataset-specific parsing and normalization logic to:\n",
    "- **Standardize nested answer formats** (e.g., from arrays or dictionaries),\n",
    "- **Escape problematic characters** (e.g., rogue quotes or newline characters),\n",
    "- **Validate presence of required fields** (`id`, `title`, `context`, `question`, `answers`),\n",
    "- **Log and isolate failures** in a separate `*_failed.csv` file for inspection.\n",
    "\n",
    "Key features:\n",
    "- Handling of inconsistencies across datasets with diverse schemas (e.g., FEVER, HotpotQA, SQuAD).\n",
    "- Inline cleaning functions for each dataset ensure modular, extensible preprocessing logic.\n",
    "- All successfully cleaned rows are written to `/data/clean/`, and any rows with malformed or incomplete data are written to `/data/raw/*_failed.csv`.\n",
    "    - The logic was used exclusively for **SQuAD v2.0** during implementation as it was the most problematic to convert from raw to a cleaned version\n",
    "\n",
    "> **Note:** This step is essential before loading data into BigQuery, as unescaped quotes and inconsistent schemas will cause ingestion to fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fd725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to JSONL\n",
    "# Clean JSONL data structure for BigQuery upload\n",
    "# Clean CSV data structure for BigQuery upload\n",
    "\n",
    "run_cleaner()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c5f94",
   "metadata": {},
   "source": [
    "## Upload Cleaned Data to BigQuery\n",
    "\n",
    "Once datasets have been cleaned and standardized, the final step is to upload them to a centralized BigQuery dataset for easy access in cloud-based analysis and modeling workflows.\n",
    "\n",
    "The `DataUploader` class manages this process, handling table creation and data ingestion. Each CSV in the `/data/clean/` directory is read and pushed to a BigQuery table under the specified dataset (`data_clean` by default), with the table name matching the file name.\n",
    "\n",
    "This enables a seamless transition from local data wrangling to scalable, cloud-native machine learning development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e666a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataUploader with BigQuery client and project ID\n",
    "uploader = DataUploader(\n",
    "    bq_client=bq_client,\n",
    "    project_id=project_id,\n",
    "    dataset_name=\"data_clean\"\n",
    ")\n",
    "\n",
    "# Upload files to BigQuery\n",
    "for path in Path(\"../data/clean\").glob(\"*.*\"):\n",
    "    name = path.stem\n",
    "    uploader.upload_to_bigquery(\n",
    "        file_path=path,\n",
    "        table_name=name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c4f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping URLs:   0%|          | 0/790 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping URLs:   2%|▏         | 16/790 [00:09<08:59,  1.44it/s]ERROR:root:[fetch_page] Error fetching https://www.mayoclinichealthsystem.org/hometown-health/speaking-of-health/can-wet-hair-make-you-sick: 403 Client Error: Forbidden for url: https://www.mayoclinichealthsystem.org/hometown-health/speaking-of-health/can-wet-hair-make-you-sick\n",
      "WARNING:root:Error processing https://www.mayoclinichealthsystem.org/hometown-health/speaking-of-health/can-wet-hair-make-you-sick: expected string or bytes-like object, got 'coroutine'\n",
      "C:\\Users\\iauge\\Documents\\Drexel MSDS\\DSCI 591\\DSCI591-FACTS\\data_acquisition\\web_scraper.py:214: RuntimeWarning: coroutine 'WebScraper.scrape_dynamic_url' was never awaited\n",
      "  logging.warning(f\"Error processing {url}: {e}\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Scraping URLs:  10%|█         | 79/790 [01:01<07:05,  1.67it/s]ERROR:root:[fetch_page] Error fetching https://www.wildlifeaid.org.uk/yuletide-dangers-for-animals/: 404 Client Error: Not Found for url: https://wildlifeaid.org.uk/yuletide-dangers-for-animals/\n",
      "WARNING:root:Error processing https://www.wildlifeaid.org.uk/yuletide-dangers-for-animals/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  15%|█▌        | 119/790 [01:21<05:30,  2.03it/s]WARNING:root:Error processing https://www.forbes.com/real-time-billionaires/#326bd4ac3d78: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  21%|██        | 162/790 [01:44<01:55,  5.45it/s]WARNING:root:Error processing https://www.webmd.com/oral-health/news/20151216/are-british-teeth-really-worse-than-american-teeth: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  23%|██▎       | 179/790 [02:01<13:11,  1.30s/it]ERROR:root:[fetch_page] Error fetching https://www.cdc.gov/alcohol/fact-sheets/moderate-drinking.htm: 404 Client Error: Not Found for url: https://www.cdc.gov/alcohol/fact-sheets/moderate-drinking.htm\n",
      "WARNING:root:Error processing https://www.cdc.gov/alcohol/fact-sheets/moderate-drinking.htm: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  23%|██▎       | 180/790 [02:02<11:01,  1.08s/it]WARNING:root:Error processing https://www.webmd.com/food-recipes/features/organic-food-better: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  23%|██▎       | 181/790 [02:04<13:29,  1.33s/it]ERROR:root:[fetch_page] Error fetching https://onlinelibrary.wiley.com/doi/full/10.1038/oby.2001.113: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/full/10.1038/oby.2001.113\n",
      "WARNING:root:Error processing https://onlinelibrary.wiley.com/doi/full/10.1038/oby.2001.113: expected string or bytes-like object, got 'coroutine'\n",
      "ERROR:root:[fetch_page] Error fetching https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30371-1/fulltext: 403 Client Error: Forbidden for url: https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30371-1/fulltext\n",
      "WARNING:root:Error processing https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30371-1/fulltext: expected string or bytes-like object, got 'coroutine'\n",
      "ERROR:root:[fetch_page] Error fetching https://onlinelibrary.wiley.com/doi/10.1111/apa.13139: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/10.1111/apa.13139\n",
      "WARNING:root:Error processing https://onlinelibrary.wiley.com/doi/10.1111/apa.13139: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  27%|██▋       | 215/790 [02:32<11:40,  1.22s/it]ERROR:root:[fetch_page] Error fetching https://www.pnas.org/content/109/Supplement_1/10661: 403 Client Error: Forbidden for url: https://www.pnas.org/content/109/Supplement_1/10661\n",
      "WARNING:root:Error processing https://www.pnas.org/content/109/Supplement_1/10661: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  29%|██▊       | 226/790 [02:36<05:38,  1.67it/s]ERROR:root:[fetch_page] Error fetching https://www.smithsonianmag.com/smart-news/oxford-university-is-older-than-the-aztecs-1529607/: 403 Client Error: Forbidden for url: https://www.smithsonianmag.com/smart-news/oxford-university-is-older-than-the-aztecs-1529607/\n",
      "WARNING:root:Error processing https://www.smithsonianmag.com/smart-news/oxford-university-is-older-than-the-aztecs-1529607/: expected string or bytes-like object, got 'coroutine'\n",
      "WARNING:root:Error processing https://www.statista.com/statistics/1100388/japan-share-of-people-who-often-consume-manga-or-anime/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  41%|████▏     | 327/790 [04:30<02:35,  2.98it/s]WARNING:root:Error processing http://www.bbc.co.uk/languages/european_languages/languages/french.shtml: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  42%|████▏     | 335/790 [04:35<02:15,  3.36it/s]ERROR:root:[fetch_page] Error fetching https://www.statcan.gc.ca/eng/dai/smr08/2015/smr08_203_2015: 404 Client Error: Not Found for url: https://www.statcan.gc.ca/en/dai/smr08/2015/smr08_203_2015\n",
      "WARNING:root:Error processing https://www.statcan.gc.ca/eng/dai/smr08/2015/smr08_203_2015: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  43%|████▎     | 342/790 [04:46<10:17,  1.38s/it]ERROR:root:[fetch_page] Error fetching https://online.ucpress.edu/mp/article-abstract/36/2/135/62855/Absolute-Pitch-and-Relative-Pitch-in-Music?redirectedFrom=fulltext: 403 Client Error: Forbidden for url: https://online.ucpress.edu/mp/article-abstract/36/2/135/62855/Absolute-Pitch-and-Relative-Pitch-in-Music?redirectedFrom=fulltext\n",
      "WARNING:root:Error processing https://online.ucpress.edu/mp/article-abstract/36/2/135/62855/Absolute-Pitch-and-Relative-Pitch-in-Music?redirectedFrom=fulltext: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  44%|████▎     | 344/790 [04:51<16:26,  2.21s/it]WARNING:root:Error processing https://optimistminds.com/what-family-members-can-you-marry-in-the-uk/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  46%|████▌     | 364/790 [05:14<10:31,  1.48s/it]ERROR:root:[fetch_page] Error fetching https://www.diamonddemocracy.com/blogs/news/is-hemp-clothing-legal: 403 Client Error: Forbidden for url: https://www.diamonddemocracy.com/blogs/news/is-hemp-clothing-legal\n",
      "WARNING:root:Error processing https://www.diamonddemocracy.com/blogs/news/is-hemp-clothing-legal: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  55%|█████▍    | 431/790 [06:01<01:08,  5.22it/s]WARNING:root:Error processing https://ourworldindata.org/grapher/per-capita-meat-type?tab=table&country=CHN~USA~IND~ARG~PRT~ETH~JPN~GBR~BRA~FRA~KOR~TWN~ITA: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  55%|█████▍    | 432/790 [06:03<05:14,  1.14it/s]WARNING:root:Error processing https://ourworldindata.org/grapher/per-capita-meat-type?tab=table&country=CHN~USA~IND~ARG~PRT~ETH~JPN~GBR~BRA~FRA~KOR~TWN~ITA: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  55%|█████▍    | 433/790 [06:05<07:21,  1.24s/it]WARNING:root:Error processing https://ourworldindata.org/grapher/per-capita-meat-type?tab=table&country=CHN~USA~IND~ARG~PRT~ETH~JPN~GBR~BRA~FRA~KOR~TWN~ITA: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  55%|█████▍    | 434/790 [06:07<08:49,  1.49s/it]WARNING:root:Error processing https://ourworldindata.org/grapher/per-capita-meat-type?tab=table&country=CHN~USA~IND~ARG~PRT~ETH~JPN~GBR~BRA~FRA~KOR~TWN~ITA: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  60%|█████▉    | 473/790 [06:34<08:56,  1.69s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)'))': /articles/the-most-reliable-real-asset-inflation-hedges/\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)'))': /articles/the-most-reliable-real-asset-inflation-hedges/\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)'))': /articles/the-most-reliable-real-asset-inflation-hedges/\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)'))': /articles/the-most-reliable-real-asset-inflation-hedges/\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)'))': /articles/the-most-reliable-real-asset-inflation-hedges/\n",
      "ERROR:root:[fetch_page] Error fetching https://wealth.northerntrust.com/articles/the-most-reliable-real-asset-inflation-hedges/: HTTPSConnectionPool(host='wealth.northerntrust.com', port=443): Max retries exceeded with url: /articles/the-most-reliable-real-asset-inflation-hedges/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)')))\n",
      "WARNING:root:Error processing https://wealth.northerntrust.com/articles/the-most-reliable-real-asset-inflation-hedges/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  60%|██████    | 474/790 [06:41<16:21,  3.11s/it]c:\\Users\\iauge\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\copyreg.py:99: RuntimeWarning: coroutine 'WebScraper.scrape_dynamic_url' was never awaited\n",
      "  return cls.__new__(cls, *args)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Scraping URLs:  66%|██████▋   | 525/790 [07:27<06:23,  1.45s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.ag.gov.au', port=443): Read timed out. (read timeout=10)\")': /legal-system/legal-assistance/national-self-representation-service\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.ag.gov.au', port=443): Read timed out. (read timeout=10)\")': /legal-system/legal-assistance/national-self-representation-service\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.ag.gov.au', port=443): Read timed out. (read timeout=10)\")': /legal-system/legal-assistance/national-self-representation-service\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.ag.gov.au', port=443): Read timed out. (read timeout=10)\")': /legal-system/legal-assistance/national-self-representation-service\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.ag.gov.au', port=443): Read timed out. (read timeout=10)\")': /legal-system/legal-assistance/national-self-representation-service\n",
      "ERROR:root:[fetch_page] Error fetching https://www.ag.gov.au/legal-system/legal-assistance/national-self-representation-service: HTTPSConnectionPool(host='www.ag.gov.au', port=443): Max retries exceeded with url: /legal-system/legal-assistance/national-self-representation-service (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.ag.gov.au', port=443): Read timed out. (read timeout=10)\"))\n",
      "WARNING:root:Error processing https://www.ag.gov.au/legal-system/legal-assistance/national-self-representation-service: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  68%|██████▊   | 537/790 [08:35<03:12,  1.31it/s]  c:\\Users\\iauge\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lxml\\html\\__init__.py:714: RuntimeWarning: coroutine 'WebScraper.scrape_dynamic_url' was never awaited\n",
      "  def lookup(self, node_type, document, namespace, name):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Scraping URLs:  77%|███████▋  | 610/790 [09:16<00:50,  3.58it/s]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.dmlp.org', port=443): Read timed out. (read timeout=10)\")': /legal-guide/opinion-and-fair-comment-privileges\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.dmlp.org', port=443): Read timed out. (read timeout=10)\")': /legal-guide/opinion-and-fair-comment-privileges\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.dmlp.org', port=443): Read timed out. (read timeout=10)\")': /legal-guide/opinion-and-fair-comment-privileges\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.dmlp.org', port=443): Read timed out. (read timeout=10)\")': /legal-guide/opinion-and-fair-comment-privileges\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.dmlp.org', port=443): Read timed out. (read timeout=10)\")': /legal-guide/opinion-and-fair-comment-privileges\n",
      "ERROR:root:[fetch_page] Error fetching https://www.dmlp.org/legal-guide/opinion-and-fair-comment-privileges: HTTPSConnectionPool(host='www.dmlp.org', port=443): Max retries exceeded with url: /legal-guide/opinion-and-fair-comment-privileges (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.dmlp.org', port=443): Read timed out. (read timeout=10)\"))\n",
      "WARNING:root:Error processing https://www.dmlp.org/legal-guide/opinion-and-fair-comment-privileges: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  78%|███████▊  | 613/790 [10:26<28:00,  9.50s/it]c:\\Users\\iauge\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\cookiejar.py:1678: RuntimeWarning: coroutine 'WebScraper.scrape_dynamic_url' was never awaited\n",
      "  self._cookies_lock.release()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "ERROR:root:[fetch_page] Error fetching https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/: 403 Client Error: Forbidden for url: https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/\n",
      "WARNING:root:Error processing https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  79%|███████▉  | 628/790 [10:39<01:20,  2.01it/s]ERROR:root:[fetch_page] Error fetching https://www.cdc.gov/niosh/topics/snakes/symptoms.html: 404 Client Error: Not Found for url: https://www.cdc.gov/niosh/topics/snakes/symptoms.html\n",
      "WARNING:root:Error processing https://www.cdc.gov/niosh/topics/snakes/symptoms.html: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  84%|████████▎ | 660/790 [11:08<01:05,  1.97it/s]ERROR:root:[fetch_page] Error fetching https://www.smithsonianmag.com/arts-culture/true-colors-17888/: 403 Client Error: Forbidden for url: https://www.smithsonianmag.com/arts-culture/true-colors-17888/\n",
      "WARNING:root:Error processing https://www.smithsonianmag.com/arts-culture/true-colors-17888/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  86%|████████▌ | 678/790 [11:33<02:37,  1.41s/it]WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))': /~norman/papers/probability_puzzles/likely_events_never_happen.html\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))': /~norman/papers/probability_puzzles/likely_events_never_happen.html\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))': /~norman/papers/probability_puzzles/likely_events_never_happen.html\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))': /~norman/papers/probability_puzzles/likely_events_never_happen.html\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))': /~norman/papers/probability_puzzles/likely_events_never_happen.html\n",
      "ERROR:root:[fetch_page] Error fetching https://www.eecs.qmul.ac.uk/~norman/papers/probability_puzzles/likely_events_never_happen.html: HTTPSConnectionPool(host='www.eecs.qmul.ac.uk', port=443): Max retries exceeded with url: /~norman/papers/probability_puzzles/likely_events_never_happen.html (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')))\n",
      "WARNING:root:Error processing https://www.eecs.qmul.ac.uk/~norman/papers/probability_puzzles/likely_events_never_happen.html: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  86%|████████▌ | 680/790 [11:40<04:13,  2.30s/it]c:\\Users\\iauge\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\copy.py:247: RuntimeWarning: coroutine 'WebScraper.scrape_dynamic_url' was never awaited\n",
      "  def _reconstruct(x, memo, func, args,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Scraping URLs:  88%|████████▊ | 693/790 [11:57<01:13,  1.33it/s]ERROR:root:[fetch_page] Error fetching https://www.smithsonianmag.com/smart-news/study-shows-knights-were-pretty-spry-their-suits-armor-180959699/: 403 Client Error: Forbidden for url: https://www.smithsonianmag.com/smart-news/study-shows-knights-were-pretty-spry-their-suits-armor-180959699/\n",
      "WARNING:root:Error processing https://www.smithsonianmag.com/smart-news/study-shows-knights-were-pretty-spry-their-suits-armor-180959699/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  88%|████████▊ | 697/790 [12:03<01:38,  1.06s/it]ERROR:root:[fetch_page] Error fetching https://www.smithsonianmag.com/science-nature/on-dinosaur-time-65556840/: 403 Client Error: Forbidden for url: https://www.smithsonianmag.com/science-nature/on-dinosaur-time-65556840/\n",
      "WARNING:root:Error processing https://www.smithsonianmag.com/science-nature/on-dinosaur-time-65556840/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  90%|█████████ | 711/790 [12:18<01:32,  1.17s/it]ERROR:root:[fetch_page] Error fetching https://www.findlaw.com/family/domestic-violence/can-the-victim-drop-domestic-violence-charges.html: 403 Client Error: Forbidden for url: https://www.findlaw.com/family/domestic-violence/can-the-victim-drop-domestic-violence-charges.html\n",
      "WARNING:root:Error processing https://www.findlaw.com/family/domestic-violence/can-the-victim-drop-domestic-violence-charges.html: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  92%|█████████▏| 728/790 [12:36<00:44,  1.39it/s]ERROR:root:[fetch_page] Error fetching https://meatscience.org/TheMeatWeEat/topics/fresh-meat/article/2016/04/14/what-is-the-liquid-in-my-meat-package: HTTPSConnectionPool(host='meatscience.org', port=443): Max retries exceeded with url: /TheMeatWeEat/topics/fresh-meat/article/2016/04/14/what-is-the-liquid-in-my-meat-package (Caused by ResponseError('too many 500 error responses'))\n",
      "WARNING:root:Error processing https://meatscience.org/TheMeatWeEat/topics/fresh-meat/article/2016/04/14/what-is-the-liquid-in-my-meat-package: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  93%|█████████▎| 735/790 [12:54<01:26,  1.57s/it]WARNING:root:Error processing https://www.nhs.uk/live-well/eat-well/never-wash-raw-chicken/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  96%|█████████▌| 759/790 [13:24<00:45,  1.48s/it]ERROR:root:[fetch_page] Error fetching https://www.americanbar.org/news/abanews/publications/youraba/2018/december-2018/when-is-it-okay-for-a-lawyer-to-lie--/: 403 Client Error: Forbidden for url: https://www.americanbar.org/news/abanews/publications/youraba/2018/december-2018/when-is-it-okay-for-a-lawyer-to-lie--/\n",
      "WARNING:root:Error processing https://www.americanbar.org/news/abanews/publications/youraba/2018/december-2018/when-is-it-okay-for-a-lawyer-to-lie--/: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs:  97%|█████████▋| 766/790 [13:31<00:22,  1.05it/s]ERROR:root:[fetch_page] Error fetching https://www.cdc.gov/tobacco/data_statistics/fact_sheets/health_effects/effects_cig_smoking/index.htm: 404 Client Error: Not Found for url: https://www.cdc.gov/tobacco/data_statistics/fact_sheets/health_effects/effects_cig_smoking/index.htm\n",
      "WARNING:root:Error processing https://www.cdc.gov/tobacco/data_statistics/fact_sheets/health_effects/effects_cig_smoking/index.htm: expected string or bytes-like object, got 'coroutine'\n",
      "Scraping URLs: 100%|██████████| 790/790 [13:41<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "clean_path = Path(\"../data/clean\")\n",
    "\n",
    "df = pd.read_csv(clean_path / \"truthful_qa_train.csv\")\n",
    "\n",
    "scraper = WebScraper(delay=2) \n",
    "\n",
    "df_augmented = await scraper.augment_dataset(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06b8626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 rows removed from dataset for lack of source text.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Flag empty or suspicious rows\n",
    "problematic_rows = df_augmented[\n",
    "    df_augmented['source_text'].isna() |\n",
    "    (df_augmented['source_text'].str.len() < 200)\n",
    "]\n",
    "\n",
    "problematic_rows.to_csv(clean_path / \"truthfulqa_missing_urls.csv\", index=False)\n",
    "print(f\"{len(problematic_rows)} rows removed from dataset for lack of source text.\")\n",
    "\n",
    "df_cleaned = df_augmented.drop(problematic_rows.index)\n",
    "\n",
    "df_cleaned.to_csv(clean_path / \"truthful_qa_with_source_text.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
