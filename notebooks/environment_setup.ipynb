{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4db8874",
   "metadata": {},
   "source": [
    "# Environment Setup & Authentication\n",
    "\n",
    "Welcome to the project onboarding notebook. This notebook helps you configure your local environment and validate access to required services such as Google Cloud (BigQuery, Cloud Storage).\n",
    "\n",
    "One key step in this setup involves authenticating with Google Cloud using a **Service Account**. Each teammate will need access to a JSON key file for the project's service account that authenticates their access to shared cloud resources.\n",
    "\n",
    "Your service account key file should be placed in the credentials folder. This keeps sensitive files organized and makes it easier to manage your environment setup across machines or users.\n",
    "\n",
    "> **Important:** Never commit your service account JSON file to version control. The `.gitignore` includes all file found within the `credentials/` directory so `.env` files and the JSON key files will not be pushed to the public repo.\n",
    "\n",
    "The next section will create and validate a `.env` file that stores the path to your service account credentials and confirms successful authentication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f0233",
   "metadata": {},
   "source": [
    "## GCP Authentication & `.env` Setup\n",
    "\n",
    "This code block does the following:\n",
    "\n",
    "1. Checks whether `credentials/secrets.env` exists.\n",
    "2. If missing, it creates a **template** with a placeholder for your service account key.\n",
    "3. It then attempts to load the environment variable `GOOGLE_APPLICATION_CREDENTIALS` from the file.\n",
    "4. If a valid path is found and the file exists, it initializes your GCP clients (BigQuery, Cloud Storage) and prints your authenticated service account email.\n",
    "\n",
    "> If the `.env` file is missing, the script will create it and **halt execution**, allowing you to add your credentials before continuing. Once the `.env` file is created, add the full path to your JSON key file which should also be stored in the `credentials/` directory.\n",
    "\n",
    "Once authenticated, you can begin querying BigQuery or interacting with GCS buckets programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c61f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from google.cloud import storage, bigquery\n",
    "from google.auth import default\n",
    "from data_pipeline.uploader import DataUploader\n",
    "from data_acquisition.loader import main as run_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540c36cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing secrets file at: C:\\Users\\iauge\\Documents\\Drexel MSDS\\DSCI 591\\DSCI591-FACTS\\credentials\\secrets.env\n",
      "GOOGLE_APPLICATION_CREDENTIALS loaded from .env\n",
      "Authenticated as: 13742792432-compute@developer.gserviceaccount.com\n",
      "GCP Project ID: dsci-591-capstone\n",
      "GCP region set to: us-east1\n"
     ]
    }
   ],
   "source": [
    "# GCP Authentication & `.env` Setup\n",
    "# This script sets up Google Cloud authentication and checks for the necessary environment variables.\n",
    "\n",
    "# Define secrets file path\n",
    "secrets_path = Path(\"../credentials/secrets.env\")\n",
    "\n",
    "# Create file if it doesn't exist\n",
    "if not secrets_path.exists():\n",
    "    print(\"'secrets.env' not found. Creating a template...\")\n",
    "    secrets_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    secrets_path.write_text(\"GOOGLE_APPLICATION_CREDENTIALS=path/to/your/service_account.json\\n\")\n",
    "    print(f\"Created template at: {secrets_path.resolve()}\")\n",
    "    print(\"Please update this file with the directory path to your GCP JSON key.\")\n",
    "    print(\"Store JSON key in the 'credentials' directory to prevent upload to GitHub.\")\n",
    "    sys.exit(1)  \n",
    "else:\n",
    "    print(f\"Found existing secrets file at: {secrets_path.resolve()}\")\n",
    "    \n",
    "load_dotenv(find_dotenv(\"../credentials/secrets.env\"))\n",
    "\n",
    "cred_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "if not cred_path or not os.path.exists(cred_path):\n",
    "    print(\n",
    "        \"GOOGLE_APPLICATION_CREDENTIALS is not set or the file does not exist.\\n\"\n",
    "        \"Please ensure secrets.env contains a valid path to your service account JSON file.\"\n",
    "    )\n",
    "    storage_client = None\n",
    "    bq_client = None\n",
    "else:\n",
    "    print(\"GOOGLE_APPLICATION_CREDENTIALS loaded from .env\")\n",
    "\n",
    "    # Initialize GCP clients using ADC\n",
    "    storage_client = storage.Client()\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Confirm authentication\n",
    "    creds, project_id = default()\n",
    "    member_email = creds.service_account_email\n",
    "    print(f\"Authenticated as: {member_email}\")\n",
    "    print(f\"GCP Project ID: {project_id}\")\n",
    "\n",
    "# GCP configuration\n",
    "REGION = \"us-east1\"\n",
    "print(f\"GCP region set to: {REGION}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae996dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize GCP clients with the project ID if provided\n",
    "storage_client = storage.Client(project = project_id if project_id else None)\n",
    "bq_client = bigquery.Client(project = project_id if project_id else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c865806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets from URLs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "URL Downloads:  20%|██        | 1/5 [00:46<03:07, 46.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ..\\data\\raw\\hotpot_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "URL Downloads:  40%|████      | 2/5 [00:51<01:05, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ..\\data\\raw\\hotpot_dev_distractor.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "URL Downloads:  60%|██████    | 3/5 [00:55<00:27, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ..\\data\\raw\\hotpot_dev_fullwiki.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "URL Downloads:  80%|████████  | 4/5 [00:55<00:08,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ..\\data\\raw\\fever_dev_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "URL Downloads: 100%|██████████| 5/5 [00:55<00:00, 11.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ..\\data\\raw\\truthful_qa_train.csv\n",
      "\n",
      "Downloading datasets from Hugging Face...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hugging Face Downloads:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a838d68ed334f1da3ab1a9d84a11d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/131 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hugging Face Downloads:  50%|█████     | 1/2 [00:06<00:06,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to ..\\data\\raw\\squad_v2_train.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4db6fd79c24b5a8dfe7d413375a4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/88 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hugging Face Downloads: 100%|██████████| 2/2 [00:10<00:00,  5.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to ..\\data\\raw\\nq_open_train.json\n",
      "\n",
      "Converting JSON files to JSONL format...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting JSON to JSONL:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting hotpot_dev_distractor.json to JSONL format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting JSON to JSONL:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ..\\data\\raw\\hotpot_dev_distractor.json to ..\\data\\raw\\hotpot_dev_distractor.jsonl\n",
      "Converting hotpot_dev_fullwiki.json to JSONL format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting JSON to JSONL:  50%|█████     | 2/4 [00:01<00:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ..\\data\\raw\\hotpot_dev_fullwiki.json to ..\\data\\raw\\hotpot_dev_fullwiki.jsonl\n",
      "Converting hotpot_train.json to JSONL format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting JSON to JSONL:  50%|█████     | 2/4 [00:10<00:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ..\\data\\raw\\hotpot_train.json to ..\\data\\raw\\hotpot_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting JSON to JSONL: 100%|██████████| 4/4 [00:10<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting nq_open_train.json to JSONL format\n",
      "Failed to convert ..\\data\\raw\\nq_open_train.json to JSONL: Extra data: line 2 column 1 (char 90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(str(Path(\"..\")))\n",
    "\n",
    "run_loader(force=False, prompt_user=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8185c706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing evidence: 19998it [00:00, 60983.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved cleaned JSONL to: ..\\data\\raw\\fever_dev_train_cleaned.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_fever_evidence(record: dict) -> list[dict]:\n",
    "    normalized = []\n",
    "    for evidence_group in record.get(\"evidence\", []):\n",
    "        for item in evidence_group:\n",
    "            if isinstance(item, list) and len(item) == 4:\n",
    "                normalized.append({\n",
    "                    \"annotation_id\": item[0],\n",
    "                    \"evidence_id\": item[1],\n",
    "                    \"wikipedia_title\": item[2],\n",
    "                    \"sentence_id\": item[3]\n",
    "                })\n",
    "    return normalized\n",
    "\n",
    "# Paths\n",
    "input_path = Path(\"../data/raw/fever_dev_train.jsonl\")\n",
    "output_path = Path(\"../data/raw/fever_dev_train_cleaned.jsonl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f_in, open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for i, line in enumerate(tqdm(f_in, desc=\"Normalizing evidence\")):\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            if \"evidence\" in record:\n",
    "                record[\"evidence\"] = normalize_fever_evidence(record)\n",
    "            json.dump(record, f_out)\n",
    "            f_out.write(\"\\n\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"⚠️ Skipped line {i+1}: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Saved cleaned JSONL to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "581fd725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 91198, 'verifiable': 'NOT VERIFIABLE', 'label': 'NOT ENOUGH INFO', 'claim': 'Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.', 'evidence': [{'annotation_id': 108548, 'evidence_id': None, 'wikipedia_title': None, 'sentence_id': None}]}\n",
      "{'id': 194462, 'verifiable': 'NOT VERIFIABLE', 'label': 'NOT ENOUGH INFO', 'claim': 'Tilda Swinton is a vegan.', 'evidence': [{'annotation_id': 227768, 'evidence_id': None, 'wikipedia_title': None, 'sentence_id': None}]}\n",
      "{'id': 137334, 'verifiable': 'VERIFIABLE', 'label': 'SUPPORTS', 'claim': 'Fox 2000 Pictures released the film Soul Food.', 'evidence': [{'annotation_id': 289914, 'evidence_id': 283015, 'wikipedia_title': 'Soul_Food_-LRB-film-RRB-', 'sentence_id': 0}, {'annotation_id': 291259, 'evidence_id': 284217, 'wikipedia_title': 'Soul_Food_-LRB-film-RRB-', 'sentence_id': 0}, {'annotation_id': 293412, 'evidence_id': 285960, 'wikipedia_title': 'Soul_Food_-LRB-film-RRB-', 'sentence_id': 0}, {'annotation_id': 337212, 'evidence_id': 322620, 'wikipedia_title': 'Soul_Food_-LRB-film-RRB-', 'sentence_id': 0}, {'annotation_id': 337214, 'evidence_id': 322622, 'wikipedia_title': 'Soul_Food_-LRB-film-RRB-', 'sentence_id': 0}]}\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, \"r\") as f:\n",
    "    for _ in range(3):\n",
    "        print(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"../data/raw/fever_dev_train.jsonl\")\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            print(f\"\\n--- Record {i+1} ---\\n\")\n",
    "            for key, val in record.items():\n",
    "                print(f\"{key}: {type(val)} → {val}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ JSON parsing error on line {i+1}: {e}\")\n",
    "        if i == 2:\n",
    "            break  # Just show first 3 lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e666a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataUploader with BigQuery client and project ID\n",
    "uploader = DataUploader(\n",
    "    bq_client=bq_client,\n",
    "    project_id=project_id,\n",
    "    dataset_name=\"data_raw\"\n",
    ")\n",
    "\n",
    "# Upload files to BigQuery\n",
    "for path in Path(\"../data/raw\").glob(\"*.*\"):\n",
    "    name = path.stem\n",
    "    uploader.upload_to_bigquery(\n",
    "        file_path=path,\n",
    "        table_name=name\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
