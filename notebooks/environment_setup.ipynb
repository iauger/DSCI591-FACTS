{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4db8874",
   "metadata": {},
   "source": [
    "# Environment Setup, Authentication and Acquisition\n",
    "\n",
    "Welcome to the project onboarding notebook. This notebook helps you configure your local environment and validate access to required services such as Google Cloud (BigQuery, Cloud Storage).\n",
    "\n",
    "One key step in this setup involves authenticating with Google Cloud using a **Service Account**. Each teammate will need access to a JSON key file for the project's service account that authenticates their access to shared cloud resources.\n",
    "\n",
    "Your service account key file should be placed in the credentials folder. This keeps sensitive files organized and makes it easier to manage your environment setup across machines or users.\n",
    "\n",
    "> **Important:** Never commit your service account JSON file to version control. The `.gitignore` includes all file found within the `credentials/` directory so `.env` files and the JSON key files will not be pushed to the public repo.\n",
    "\n",
    "The next section will create and validate a `.env` file that stores the path to your service account credentials and confirms successful authentication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f0233",
   "metadata": {},
   "source": [
    "## GCP Authentication & `.env` Setup\n",
    "\n",
    "This code block does the following:\n",
    "\n",
    "1. Checks whether `credentials/secrets.env` exists.\n",
    "2. If missing, it creates a **template** with a placeholder for your service account key.\n",
    "3. It then attempts to load the environment variable `GOOGLE_APPLICATION_CREDENTIALS` from the file.\n",
    "4. If a valid path is found and the file exists, it initializes your GCP clients (BigQuery, Cloud Storage) and prints your authenticated service account email.\n",
    "\n",
    "> If the `.env` file is missing, the script will create it and **halt execution**, allowing you to add your credentials before continuing. Once the `.env` file is created, add the full path to your JSON key file which should also be stored in the `credentials/` directory.\n",
    "\n",
    "Once authenticated, you can begin querying BigQuery or interacting with GCS buckets programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c61f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from google.cloud import storage, bigquery\n",
    "from google.auth import default\n",
    "from data_pipeline.uploader import DataUploader\n",
    "from data_acquisition.loader import main as run_loader\n",
    "from data_acquisition.data_cleaner import main as run_cleaner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c36cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing secrets file at: C:\\Users\\iauge\\Documents\\Drexel MSDS\\DSCI 591\\DSCI591-FACTS\\credentials\\secrets.env\n",
      "GOOGLE_APPLICATION_CREDENTIALS loaded from .env\n",
      "Authenticated as: 13742792432-compute@developer.gserviceaccount.com\n",
      "GCP Project ID: dsci-591-capstone\n",
      "GCP region set to: us-east1\n"
     ]
    }
   ],
   "source": [
    "# GCP Authentication & `.env` Setup\n",
    "# This script sets up Google Cloud authentication and checks for the necessary environment variables.\n",
    "\n",
    "# Ensure credentials directory exists\n",
    "credentials_dir = Path(\"../credentials\")\n",
    "\n",
    "if not credentials_dir.exists():\n",
    "    print(\"Credentials directory not found. Creating...\")\n",
    "    credentials_dir.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    print(f\"Credentials directory found at: {credentials_dir.resolve()}\")\n",
    "    \n",
    "# Define secrets file path\n",
    "secrets_path = Path(\"../credentials/secrets.env\")\n",
    "\n",
    "# Create file if it doesn't exist\n",
    "if not secrets_path.exists():\n",
    "    print(\"'secrets.env' not found. Creating a template...\")\n",
    "    secrets_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    secrets_path.write_text(\"GOOGLE_APPLICATION_CREDENTIALS=path/to/your/service_account.json\\n\")\n",
    "    print(f\"Created template at: {secrets_path.resolve()}\")\n",
    "    print(\"Please update this file with the directory path to your GCP JSON key.\")\n",
    "    print(\"Store JSON key in the 'credentials' directory to prevent upload to GitHub.\")\n",
    "    sys.exit(1)  \n",
    "else:\n",
    "    print(f\"Found existing secrets file at: {secrets_path.resolve()}\")\n",
    "    \n",
    "load_dotenv(find_dotenv(\"../credentials/secrets.env\"))\n",
    "\n",
    "cred_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "if not cred_path or not os.path.exists(cred_path):\n",
    "    print(\n",
    "        \"GOOGLE_APPLICATION_CREDENTIALS is not set or the file does not exist.\\n\"\n",
    "        \"Please ensure secrets.env contains a valid path to your service account JSON file.\"\n",
    "    )\n",
    "    storage_client = None\n",
    "    bq_client = None\n",
    "else:\n",
    "    print(\"GOOGLE_APPLICATION_CREDENTIALS loaded from .env\")\n",
    "\n",
    "    # Initialize GCP clients using ADC\n",
    "    storage_client = storage.Client()\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Confirm authentication\n",
    "    creds, project_id = default()\n",
    "    member_email = creds.service_account_email\n",
    "    print(f\"Authenticated as: {member_email}\")\n",
    "    print(f\"GCP Project ID: {project_id}\")\n",
    "\n",
    "# GCP configuration\n",
    "REGION = \"us-east1\"\n",
    "print(f\"GCP region set to: {REGION}\")\n",
    "\n",
    "# Initialize GCP clients with the project ID if provided\n",
    "storage_client = storage.Client(project = project_id if project_id else None)\n",
    "bq_client = bigquery.Client(project = project_id if project_id else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753d6a2",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "This section outlines the process used to gather, standardize, and prepare a diverse set of question-answering (QA) datasets for downstream machine learning tasks.\n",
    "\n",
    "Our goal is to build a robust and scalable pipeline for retrieving raw datasets, performing initial validation and cleaning, and outputting schema-consistent files ready for preprocessing, exploration, feature extraction, and modeling.\n",
    "\n",
    "### Approach Overview\n",
    "\n",
    "The pipeline is composed of three main stages:\n",
    "\n",
    "1. **Data Loading** (`loader.py`):  \n",
    "   Downloads raw datasets from either direct URLs or the Hugging Face Hub. All files are stored in `/data/raw/` in their original formats (e.g., JSONL, CSV, Parquet).\n",
    "\n",
    "2. **Data Cleaning** (`cleaner.py`):  \n",
    "   Transforms raw files into clean, flat CSVs with standardized fields required for QA tasks: `id`, `title`, `context`, `question`, and `answers`. Rows with formatting or structural issues are logged separately for inspection.\n",
    "\n",
    "3. **Data Upload** (`#TODO - script in progress`):  \n",
    "   A final upload step will push cleaned datasets to BigQuery for centralized cloud storage, enabling streamlined access to modeling workflows using Google's ML tools, including Vertex AI.\n",
    "\n",
    "The pipeline is modular by design. New datasets can be added by extending the loader configuration and creating a dataset-specific cleaner as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fb449",
   "metadata": {},
   "source": [
    "## Run the Data Loading Script\n",
    "\n",
    "The `loader.py` script is responsible for downloading and storing a core set of fact verification and QA datasets into the local project environment in their **original file formats** (e.g., `.json`, `.jsonl`, `.parquet`, or `.csv`).\n",
    "\n",
    "Currently supported datasets include:\n",
    "- **FEVER 2.0**\n",
    "- **HotpotQA**\n",
    "- **Natural Questions (Lite)**\n",
    "- **SQuAD v2.0**\n",
    "- **TruthfulQA**\n",
    "\n",
    "The script is built around a modular `DataDownloader` class, which encapsulates:\n",
    "- dataset-specific retrieval logic,\n",
    "- support for both **Hugging Face Hub** and **direct download URLs**,\n",
    "- dynamic filetype handling for JSON, JSONL, CSV, and Parquet,\n",
    "- customizable storage paths.\n",
    "\n",
    "This design makes it easy to extend with new datasets: simply update the Hugging Face or URL mappings in `loader.py`, and rerun the script. Each dataset is downloaded only once unless the `overwrite` flag is enabled.\n",
    "\n",
    "> **Note:** All files are saved into the `/data/raw/` folder using consistent and identifiable filenames to support reproducibility and transparent data lineage in downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c865806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download raw data files from URLs and Hugging Face\n",
    "\n",
    "run_loader(force=False, prompt_user=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b054b13",
   "metadata": {},
   "source": [
    "## Run the Dataset Cleaning Script\n",
    "\n",
    "The `cleaner.py` script processes raw QA datasets from the `/data/raw/` directory and transforms them into clean, BigQuery-compatible CSV files stored in `/data/clean/`.\n",
    "\n",
    "This script uses the `DataCleaner` class, which includes dataset-specific parsing and normalization logic to:\n",
    "- **Standardize nested answer formats** (e.g., from arrays or dictionaries),\n",
    "- **Escape problematic characters** (e.g., rogue quotes or newline characters),\n",
    "- **Validate presence of required fields** (`id`, `title`, `context`, `question`, `answers`),\n",
    "- **Log and isolate failures** in a separate `*_failed.csv` file for inspection.\n",
    "\n",
    "Key features:\n",
    "- Handling of inconsistencies across datasets with diverse schemas (e.g., FEVER, HotpotQA, SQuAD).\n",
    "- Inline cleaning functions for each dataset ensure modular, extensible preprocessing logic.\n",
    "- All successfully cleaned rows are written to `/data/clean/`, and any rows with malformed or incomplete data are written to `/data/raw/*_failed.csv`.\n",
    "    - The logic was used exclusively for **SQuAD v2.0** during implementation as it was the most problematic to convert from raw to a cleaned version\n",
    "\n",
    "> **Note:** This step is essential before loading data into BigQuery, as unescaped quotes and inconsistent schemas will cause ingestion to fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581fd725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning datasets: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing JSONL file: fever_dev_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning fever_dev_train.jsonl: 19998it [00:00, 62420.79it/s]\n",
      "Cleaning datasets: 1it [00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting hotpot_dev_distractor.json to JSONL format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning datasets: 1it [00:01,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ..\\data\\raw\\hotpot_dev_distractor.json to ..\\data\\raw\\hotpot_dev_distractor.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning hotpot_dev_distractor.jsonl: 7405it [00:01, 6438.10it/s]\n",
      "Cleaning datasets: 2it [00:02,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing JSONL file: hotpot_dev_distractor.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning hotpot_dev_distractor.jsonl: 7405it [00:01, 6382.12it/s]\n",
      "Cleaning datasets: 3it [00:03,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting hotpot_dev_fullwiki.json to JSONL format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning datasets: 3it [00:04,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ..\\data\\raw\\hotpot_dev_fullwiki.json to ..\\data\\raw\\hotpot_dev_fullwiki.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning hotpot_dev_fullwiki.jsonl: 7405it [00:01, 6187.41it/s]\n",
      "Cleaning datasets: 4it [00:05,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing JSONL file: hotpot_dev_fullwiki.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning hotpot_dev_fullwiki.jsonl: 7405it [00:01, 6265.23it/s]\n",
      "Cleaning datasets: 5it [00:06,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting hotpot_train.json to JSONL format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning datasets: 5it [00:15,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ..\\data\\raw\\hotpot_train.json to ..\\data\\raw\\hotpot_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning hotpot_train.jsonl: 90447it [00:14, 6291.31it/s]\n",
      "Cleaning datasets: 6it [00:30,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing JSONL file: hotpot_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning hotpot_train.jsonl: 90447it [00:14, 6319.75it/s]\n",
      "Cleaning datasets: 7it [00:45, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting nq_open_train.json to JSONL format\n",
      "JSON decode failed with: Extra data: line 2 column 1 (char 90). Assuming file is already JSONL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning nq_open_train.jsonl: 87925it [00:00, 125304.03it/s]\n",
      "Cleaning datasets: 8it [00:46,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing JSONL file: nq_open_train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning nq_open_train.jsonl: 87925it [00:00, 127958.67it/s]\n",
      "Cleaning datasets: 9it [00:46,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CSV file: squad_v2_train.csv\n",
      "Cleaning SQuAD CSV: squad_v2_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning datasets: 10it [01:23, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned CSV to: ..\\data\\clean\\squad_v2_train.csv\n",
      "Saved 109 failed rows to: ..\\data\\raw\\squad_v2_train_failed.csv\n",
      "Processing CSV file: squad_v2_validation.csv\n",
      "Cleaning SQuAD CSV: squad_v2_validation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning datasets: 14it [01:26,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned CSV to: ..\\data\\clean\\squad_v2_validation.csv\n",
      "Saved 11 failed rows to: ..\\data\\raw\\squad_v2_validation_failed.csv\n",
      "Processing CSV file: truthful_qa_train.csv\n",
      "No specific cleaning function for truthful_qa_train.csv, copying as is.\n",
      "Data cleaning completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert JSON to JSONL\n",
    "# Clean JSONL data structure for BigQuery upload\n",
    "# Clean CSV data structure for BigQuery upload\n",
    "\n",
    "run_cleaner()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c5f94",
   "metadata": {},
   "source": [
    "## Upload Cleaned Data to BigQuery\n",
    "\n",
    "Once datasets have been cleaned and standardized, the final step is to upload them to a centralized BigQuery dataset for easy access in cloud-based analysis and modeling workflows.\n",
    "\n",
    "The `DataUploader` class manages this process, handling table creation and data ingestion. Each CSV in the `/data/clean/` directory is read and pushed to a BigQuery table under the specified dataset (`data_clean` by default), with the table name matching the file name.\n",
    "\n",
    "This enables a seamless transition from local data wrangling to scalable, cloud-native machine learning development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e666a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dsci-591-capstone.data_clean.fever_dev_train already exists. Using existing table.\n",
      "Successfully uploaded ..\\data\\clean\\fever_dev_train.jsonl to BigQuery table dsci-591-capstone.data_clean.fever_dev_train.\n",
      "Table dsci-591-capstone.data_clean.hotpot_dev_distractor already exists. Using existing table.\n",
      "Successfully uploaded ..\\data\\clean\\hotpot_dev_distractor.jsonl to BigQuery table dsci-591-capstone.data_clean.hotpot_dev_distractor.\n",
      "Table dsci-591-capstone.data_clean.hotpot_dev_fullwiki already exists. Using existing table.\n",
      "Successfully uploaded ..\\data\\clean\\hotpot_dev_fullwiki.jsonl to BigQuery table dsci-591-capstone.data_clean.hotpot_dev_fullwiki.\n",
      "Table dsci-591-capstone.data_clean.hotpot_train already exists. Using existing table.\n",
      "Successfully uploaded ..\\data\\clean\\hotpot_train.jsonl to BigQuery table dsci-591-capstone.data_clean.hotpot_train.\n",
      "Table dsci-591-capstone.data_clean.nq_open_train already exists. Using existing table.\n",
      "Successfully uploaded ..\\data\\clean\\nq_open_train.jsonl to BigQuery table dsci-591-capstone.data_clean.nq_open_train.\n",
      "Creating new table dsci-591-capstone.data_clean.squad_v2_train.\n",
      "Successfully uploaded ..\\data\\clean\\squad_v2_train.csv to BigQuery table dsci-591-capstone.data_clean.squad_v2_train.\n",
      "Creating new table dsci-591-capstone.data_clean.squad_v2_validation.\n",
      "Successfully uploaded ..\\data\\clean\\squad_v2_validation.csv to BigQuery table dsci-591-capstone.data_clean.squad_v2_validation.\n",
      "Table dsci-591-capstone.data_clean.truthful_qa_train already exists. Using existing table.\n",
      "Successfully uploaded ..\\data\\clean\\truthful_qa_train.csv to BigQuery table dsci-591-capstone.data_clean.truthful_qa_train.\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataUploader with BigQuery client and project ID\n",
    "uploader = DataUploader(\n",
    "    bq_client=bq_client,\n",
    "    project_id=project_id,\n",
    "    dataset_name=\"data_clean\"\n",
    ")\n",
    "\n",
    "# Upload files to BigQuery\n",
    "for path in Path(\"../data/clean\").glob(\"*.*\"):\n",
    "    name = path.stem\n",
    "    uploader.upload_to_bigquery(\n",
    "        file_path=path,\n",
    "        table_name=name\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
