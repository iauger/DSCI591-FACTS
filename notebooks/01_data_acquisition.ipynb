{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4db8874",
   "metadata": {},
   "source": [
    "# 01 · Data Acquisition\n",
    "\n",
    "**Purpose**  \n",
    "This notebook downloads all raw datasets required for the project into `../data/raw/`.  \n",
    "It acts as a wrapper for [`loader.py`](../data_acquisition/loader.py), which:\n",
    "\n",
    "- Retrieves datasets from predefined URLs\n",
    "- Downloads from Hugging Face repositories\n",
    "- Handles Parquet → CSV conversions\n",
    "- Saves all outputs to the `data/raw/` directory\n",
    "\n",
    "**Usage Notes**  \n",
    "- Run this notebook when setting up the project for the first time or when refreshing datasets.  \n",
    "- The cleaning and preprocessing of these datasets will be handled later in `02_cleaning_preprocessing.ipynb`.  \n",
    "- To overwrite existing files, set `FORCE_DOWNLOAD = True` in the settings cell below.  \n",
    "- For interactive overwrite confirmation, set `PROMPT_USER = True`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c619d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data directory: C:\\Users\\iauge\\Documents\\Drexel MSDS\\DSCI 591\\DSCI591-FACTS\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Ensure project modules are on path ---\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# --- Ensure raw data directory exists ---\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Raw data directory: {RAW_DIR.resolve()}\")\n",
    "\n",
    "# --- Import loader main ---\n",
    "from data_acquisition.loader import main as loader_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca4bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download Settings ---\n",
    "# Set to True to overwrite existing files\n",
    "FORCE_DOWNLOAD = False\n",
    "\n",
    "# Set to True to prompt interactively (forces user input in cell output)\n",
    "PROMPT_USER = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132910b1",
   "metadata": {},
   "source": [
    "## Run the Data Loading Script\n",
    "\n",
    "The `loader.py` script is responsible for downloading and storing a core set of fact verification and QA datasets into the local project environment in their **original file formats** (e.g., `.json`, `.jsonl`, `.parquet`, or `.csv`).\n",
    "\n",
    "Currently supported datasets include:\n",
    "- **FEVER 2.0**\n",
    "- **HotpotQA**\n",
    "- **Natural Questions (Lite)**\n",
    "- **SQuAD v2.0**\n",
    "- **TruthfulQA**\n",
    "\n",
    "The script is built around a modular `DataDownloader` class, which encapsulates:\n",
    "- dataset-specific retrieval logic,\n",
    "- support for both **Hugging Face Hub** and **direct download URLs**,\n",
    "- dynamic filetype handling for JSON, JSONL, CSV, and Parquet,\n",
    "- customizable storage paths.\n",
    "\n",
    "This design makes it easy to extend with new datasets: simply update the Hugging Face or URL mappings in `loader.py`, and rerun the script. Each dataset is downloaded only once unless the `overwrite` flag is enabled.\n",
    "\n",
    "> **Note:** All files are saved into the `/data/raw/` folder using consistent and identifiable filenames to support reproducibility and transparent data lineage in downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c75876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "URL Downloads: 100%|██████████| 7/7 [00:00<00:00, 699.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping hotpot_train: cleaned or downloaded file already exists.\n",
      "Skipping hotpot_dev_distractor: cleaned or downloaded file already exists.\n",
      "Skipping hotpot_dev_fullwiki: cleaned or downloaded file already exists.\n",
      "Skipping fever_dev_train: cleaned or downloaded file already exists.\n",
      "Skipping truthful_qa_train: cleaned or downloaded file already exists.\n",
      "Skipping squad_v2_train: cleaned or downloaded file already exists.\n",
      "Skipping squad_v2_validation: cleaned or downloaded file already exists.\n",
      "\n",
      "Downloading datasets from Hugging Face...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hugging Face Downloads: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping HuggingFace download for nq_open_train: file already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Run the Loader ---\n",
    "loader_main(force=FORCE_DOWNLOAD, prompt_user=PROMPT_USER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c12329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloaded files in ../data/raw/:\n",
      "fever_dev_train.jsonl  (4.15 MB)\n",
      "hotpot_dev_distractor.json  (44.17 MB)\n",
      "hotpot_dev_distractor.jsonl  (44.65 MB)\n",
      "hotpot_dev_fullwiki.json  (45.26 MB)\n",
      "hotpot_dev_fullwiki.jsonl  (45.74 MB)\n",
      "hotpot_train.json  (540.19 MB)\n",
      "hotpot_train.jsonl  (540.19 MB)\n",
      "nq_open_train.json  (7.86 MB)\n",
      "nq_open_train.jsonl  (7.86 MB)\n",
      "squad_v2_train.csv  (117.54 MB)\n",
      "squad_v2_train.parquet  (15.61 MB)\n",
      "squad_v2_train_failed.csv  (0.12 MB)\n",
      "squad_v2_validation.csv  (11.76 MB)\n",
      "squad_v2_validation.parquet  (1.29 MB)\n",
      "squad_v2_validation_failed.csv  (0.01 MB)\n",
      "truthful_qa_train.csv  (0.48 MB)\n"
     ]
    }
   ],
   "source": [
    "# --- Post-download check ---\n",
    "\n",
    "print(\"\\nDownloaded files in ../data/raw/:\")\n",
    "for f in sorted(RAW_DIR.glob(\"*\")):\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"{f.name}  ({size_mb:.2f} MB)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
